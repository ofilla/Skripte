# ASP: Tutorial 2
# 1. Langmuir absorption
> Consider a lattice of $N_s$ sites, each of which is vacant with probability $1-p$ or occupied by a single particle with probability $p$. Multiple occupancy is not allowed, i.e., the particles are subject to a hard-core interaction.

## a) $\kappa_T$
> Calculate the mean $\braket N$ and variance $\Delta N^2$ of the particle number in the system. Then use the general relation given in the lecture to determine the isothermal compressibility $\kappa_T$ of the system as a function of $T$ and the mean density $\rho=\braket{N}/N$. Compare the result to the compressibility of the ideal classical gas computed from the ideal gas law.

In equilibrium, $N=\braket{N}$ is the number of particles on the lattice. In a Binomial distribution, this is $\braket N=pN_s$ with $\Delta N^2 = (1-p)pN_s$.[^1]

[^1]: http://www.gbraemik.de/mathe/BinomialverteilungErwartungswertVarianz.pdf


The first equation is taken from the lecture. Further, the stationary density $\rho$ is equivalent to the probability $p$.

$$
\begin{eqnarray}
    \Delta N^2 &=& \frac{N^2}{V} k_BT \kappa_T \\
    \Rightarrow (1-p)pN_s &=& \frac{p^2N_s^2}{V} k_BT \kappa_T \\
    \Leftrightarrow \kappa_T &=& \frac{(1-p)V}{pN_s\cdot k_BT} \\
    \rho &=& \frac{\braket N}{N} = p \\
    \Rightarrow \kappa_T &=& \frac{(1-\rho)V}{\rho N_s\cdot k_BT}
\end{eqnarray}
$$

For the ideal gas, the Sackur-Tetrode formula $S(E,V,N)$ is known. The pressure $P$ can be derived from it.

$$
\begin{eqnarray}
    S(E, V, N) &=& k_bN
        \left[
            \frac{3}{2}\ln\left(\frac{E}{N}\right)
            + \frac{3}{2}\ln\left(\frac{V}{N}\right)
            + \ln[c]
        \right] \\
    P &=& T\left(\frac{\partial S}{\partial V}\right)_{E,N} \\
    \qq{(1)\qquad\qquad\qquad\qquad}
        &=& \frac{k_BT N}{V} \\
    \kappa_{T,\mathrm{ideal}} &=& -\frac{1}{V} \left(\frac{\partial S^2}{\partial P^2}\right)_{T,N} \\
        &=& \frac{1}{V} \cdot \frac{Nk_BT}{P^2} \\
        &=& \frac{1}{P} \\
    \qq*{with (1)\qquad\qquad\qquad\qquad}
        &=& \frac{V}{Nk_BT}
\end{eqnarray}
$$

Thus, $\kappa_{T,\mathrm{ideal}}$ and $\kappa_T$ are equal for $p=0.5$:

$$
\begin{eqnarray}
    \kappa_T &=& \kappa_{T,\mathrm{ideal}} \cdot \frac{1-\rho}{\rho}
\end{eqnarray}
$$

## b) $J_1$ and $P_1$
> The Langmuir model can be used as a model for absorption of molecules of an ideal gas on a solid substrate (here the lattice) that traps molecules with probability $p$. The ideal gas can then be seen as the reservoir of the system (the lattice) in the grand-canonical sense.
> 
> Consider a single site. What is the probability that it is occupied in the grand-canonical ensemble? Compute the grand partition function $J_1$ of a single site. Use these results to show that the the probability of occupation $p$ is related to the chemical potential $\mu$ of the system through $p = \frac{\exp(\beta\mu)}{1+\exp(\beta\mu)}$.

Probability $P_s$ and the grand partition function $J$ are defined as follows, cf. lecture. Similar to the calculation in $a)$, the equipartition law for $E$ can be calculated from the Sackur-Tetrode formula.

$$
\begin{eqnarray}
    P_s &=& \frac{1}{J} \exp(-\beta (E_s-\mu N_s)) \\
    J &=& \sum_s \exp(-\beta (E_s-\mu N_s)) \\
    E &=& \frac{3}{2} k_BTN
\end{eqnarray}
$$

For the total lattice, $J$ is denoted below, since $N=\braket N$, $N_1=1$ and $N_0=0$. For a single site, $J_1$ is the correct partition function, because the states of single sites are independent from each other. Here, only two states are possible: Either the lattice is occupied or vacant.

$$
\begin{eqnarray}
    J &=& N_sp \exp[-\beta(E_1-\mu)]
          + N_s(1-p) \exp[-\beta E_0] \\
    J_1 &=& \exp[-\beta(E_1-\mu)] + \exp[-\beta E_0]
\end{eqnarray}
$$

The energy $E_i$ can be described by the energy $E(N)$ of the complete lattice with $N$ occupied positions. For large $N_s$, the total energy is only slightly dependent on the state of a single position.

$$
\begin{eqnarray}
    E_1 &=& \frac{E(N)}{N_s} \\
    E_0 &=& \frac{E(N-1)}{N_s} \\
    \Rightarrow E_1 &\approx& E_0
\end{eqnarray}
$$

Thus, $J_1$ can be simplified even further. This leads to $p=P_1$.

$$
\begin{eqnarray}
    J_1 &\approx& \exp[-\beta(E_s-\mu)] + \exp[-\beta E_s] \\
        &=& \exp[-\beta E_s] (1+\exp[\beta \mu]) \\
    P_1 &=& \frac{1}{J_1} \exp(-\beta (E_1-\mu N_1)) \\
        &\approx& \frac{\exp(-\beta (E_1-\mu N_1))}{\exp[-\beta E_1] (1+\exp[\beta \mu])} \\
    P_1 &=& \frac{\exp(\beta\mu)}{1+\exp(\beta\mu)} \qquad \square
\end{eqnarray}
$$

Also, $P_1$ is sum over all states with the position $1$ occupied, i.e. over all valid $E_i$.

$$
\begin{eqnarray}
    P_1 &=& \sum_i \frac{1}{J} \exp(-\beta (E_i-\mu N_1)) \\
        &=& \frac{
                \sum_i \exp(-\beta (E_i-\mu N_1))
            }{
                \sum_i \exp(-\beta (E_i-\mu N_1))
                + \sum_i \exp(-\beta (E_i))
            } \\
        &=& \frac{\exp(\beta\mu)}{1+\exp(\beta\mu)} \qquad \square
\end{eqnarray}
$$

## c) $\mu_g$
> The chemical potential of an ideal gas is equal to $\mu_g$, where $\alpha=m/2\pi\hbar^2$ and $P_g$ is the pressure of the ideal gas. Use this expression to express the probability of occupation $p$ in the lattice as a function of $P_g$ and $T$. How does $p$ depend on $P_g$ and $T$? Why?
$$
\begin{eqnarray}
    \mu_g = -k_BT \ln\left(
            \frac{k_BT}{P_g} (k_BT\alpha)^{3/2}
        \right)
\end{eqnarray}
$$

Inserting $\mu_g$ into $P_1$:

$$
\begin{eqnarray}
    p &=& \frac{
                \exp\left[
                    -\ln \frac{k_BT}{P_g} (k_BT\alpha)^{3/2}
                \right]
            }{
                1 +
                \exp\left[
                    -\ln \frac{k_BT}{P_g} (k_BT\alpha)^{3/2}
                \right]
            } \\
        &=& \frac{
                P_g \left(\sqrt{k_B^5T^5\alpha^3}\right)^{-1}
                }{
                1 +
                P_g \left(\sqrt{k_B^5T^5\alpha^3}\right)^{-1}
            } \\
    p &=& \frac{
                P_g
            }{
                \sqrt{(k_BT)^5\alpha^3} + P_g
            }
\end{eqnarray}
$$

This can be analysed using limits $0$ or $\infty$.

$$
\begin{eqnarray}
    \lim_{P_g \rightarrow 0} p &=& 0 \\
    \lim_{P_g \rightarrow \infty} p &=& 1 \\
    \lim_{T \rightarrow 0} p &=& 1 \\
    \lim_{T \rightarrow \infty} p &=& 0
\end{eqnarray}
$$

For low pressure or high temperature, an empty lattice is most probable, while high pressure or low temperature enforce particles on the lattice.

This can be explained by movement of particles: Hot particle have high energy and need space to move, while frozen particles are (nearly) stationary.

# 2. Shannon entropy and information theory
> The Shannon or Gibbs entropy of a random variable $X$ with probability distribution $p_x$ defined over a discrete set of states $x$ is given by [following equation], where $k$ is an arbitrary positive constant. This entropy can be seen as the average amount of information obtained when reading outcomes of $X$.
$$
\begin{eqnarray}
    S &=& -k \sum_x p_x \ln p_x
\end{eqnarray}
$$

## a) maximizing entropy
> Show that the micro-canonical, canonical and grand-canonical distribution can be re-covered by maximizing the entropy (as a function of all $\{p_x\}$) under the constraint of fixed average energy in the canonical ensemble, and fixed average energy and particle number in the grand canonical one using the method of Lagrange multipliers.

### micro-canonical ensemble
The micro-canonical ensemble maximizes entropy when the probabilities are normalized.

$$
\begin{eqnarray}
    \sum_x p_x &=& 1 \\
    \Rightarrow g_1(p_x) &=& 1 - \sum_x p_x \\
    \Rightarrow S_\mathrm{mc}(p_x) &=& S + \lambda_1g_1(p_x)
\end{eqnarray}
$$

Maximizing $S_\mathrm{mc}$ needs the gradient to become $0$, for each $r$:

$$
\begin{eqnarray}
    \forall r: \frac{\partial S_\mathrm{sc}}{\partial p_r} &\overset{!}{=} 0 \\
    \forall r: \frac{\partial S_\mathrm{sc}}{\partial p_r}
        &=& -k(\ln p_r + 1) - \lambda_1 \\
    \Rightarrow \forall r: \ln(p_r+1) &=& -\frac{\lambda_1}{k} \\
    \Leftrightarrow p_r &=& \exp[-1 - \frac{\lambda_1}{k}]
\end{eqnarray}
$$

Since this must be valid for all $r$, all $p_r$ need to be the same. To match the usual notation, $k=k_B$ and $\Omega=N$ are to be set. This can be proven inserting $p_r$ into $g_1(p_r)$.

$$
\begin{eqnarray}
    \Rightarrow \forall r: p_r &=& \frac{1}{N} \\
    \Rightarrow \lambda_1 &=& k(\ln N - 1) \\
    \Rightarrow S_\mathrm{mc} &=& -k \sum_x p_x \ln p_x \\
        &=& -k \sum_x \frac{1}{N} \cdot (-\ln N) \\
        &=& k \ln N \\
        &=& k_B \ln \Omega
\end{eqnarray}
$$

### canonical ensemble
The canonical ensemble realizes the additional constraint of constant average energy $E$:

$$
\begin{eqnarray}
    \sum_x p_x &=& 1 \\
    \Rightarrow g_1(p_x) &=& 1 - \sum_x p_x \\
    \sum_x p_xE_x &=& E \\
    \Rightarrow g_2(p_x) &=& E - \sum_x p_xE_x \\
    \Rightarrow S_\mathrm{c}(p_x)
        &=& S + \lambda_1g_1(p_x) + \lambda_2g_2(p_x)
\end{eqnarray}
$$

Thus, $p_r$ needs to be described by the temperature $T$:

$$
\begin{eqnarray}
    \forall r: \frac{\partial S_\mathrm{c}}{\partial p_r} &\overset{!}{=} 0 \\
    \forall r: \frac{\partial S_\mathrm{c}}{\partial p_r}
        &=& -k(\ln p_r + 1) - \lambda_1 - \lambda_2 E_r \\
        &=& \frac{1}{N}\exp(-\lambda_2 E_r) \\
    \Rightarrow p_r &=& \frac{1}{N} \exp(- \frac{E_r}{kT})
\end{eqnarray}
$$

$\lambda_2$ can be calculated using $g_2$:

$$
\begin{eqnarray}
    \sum_r \frac{1}{N} \exp(- \lambda_2 E_r) &=& E \\
    &\vdots& \\
    \Rightarrow \lambda_2 &=& \frac{1}{kT}
\end{eqnarray}
$$

### grand-canonical ensemble
The grand-canonical ensemble additionally constraints the average number of particles $N$.

$$
\begin{eqnarray}
    \sum_x p_x &=& 1 \\
        \Rightarrow g_1(p_x) &=& 1 - \sum_x p_x \\
    \sum_x p_xE_x &=& E \\
        \Rightarrow g_2(p_x) &=& E - \sum_x p_xE_x \\
    \sum_x p_xN_x &=& N \\
        \Rightarrow g_3(p_x) &=& N - \sum_x p_xN_x \\
    \Rightarrow S_\mathrm{gc}(p_x)
        &=& S + \lambda_1g_1(p_x) + \lambda_2g_2(p_x) + \lambda_3g_3(p_x)
\end{eqnarray}
$$


$$
\begin{eqnarray}
    \forall r: \frac{\partial S_\mathrm{gc}}{\partial p_r} &\overset{!}{=} 0 \\
    \forall r: \frac{\partial S_\mathrm{gc}}{\partial p_r}
        &=& -k(\ln p_r + 1) - \lambda_1 - \lambda_2 E_r - \lambda_3 N_r \\
    \Rightarrow p_r &=& \frac{1}{N} \exp(- \frac{E_r}{kT} - \lambda_3 N) \\
    &\vdots& \\
    \Rightarrow \lambda_3 &=& -k\mu
\end{eqnarray}
$$

## coin-weighting puzzle
> At equilibrium, the information content is thus always maximized. Besides statistical mechanics, this definition of entropy is a very useful concept in different contexts as we shall see. We will use the notion of entropy to analyze the following puzzle: There are nine coins, and all of them are identical except one, which is slightly heavier. You can use a two-pan weighing balance to detect the defective coin. The challenge is to use the minimum  number of weightings to achieve this with certainty. From now on, we work with $S=-\sum_x p_x \log_2 p_x$, and we say that the entropy is measured in bits.

$$
\begin{eqnarray}
    S &=& \braket{-\log(P(X))} \\
    S &=& -\sum_x p_x \log_2 p_x
\end{eqnarray}
$$

### b) entropy
> In the coin-weighing puzzle, how much information, in bits, do you need to gain in order to identify the defective coin? It is useful to rephrase the problem as a random experiment.

There are $9$ coins. Each of them can have $2$ states: Either, coin $i$ is fair or it's biased. Thus, each coin carries can have $2$ states.

Still, all coins have the same amount of information. The probability to choose a random coin is the same for all coins. This uses the microcanonical ensemble, where $S$ describes the information contained.

$$
\begin{eqnarray}
    p_x &=& \frac{1}{9} \\
    S &=& - \sum_{i=1}^9 \frac{1}{9} \log_2 \frac{1}{9} \\
        &=& \log_2 9 \\
        &\approx& 3.17
\end{eqnarray}
$$

Thus, the entropy contains $4\mathrm{\,bit}$ of information When the heavy coin is found, the entropy becomes $S=0$.

### c) maximizing entropy
> Let the maximum amount of information that can be gained from a single weighing be $I$ bits. Show that the maximum averaged amount of information that can be gained from a single weighing is $I=\log_2 (3)$. From this, show that the minimum number of weightings required to identify the defective coin is $2$.

Since each weighting could yield $3$ results (either side or none is heavier), $I$ is the maximal information to be gained from a single weighting. To gain maximal information, each of these results should have the same probability.

Because there are $S$ bits of information in total and $I$ can be gained in one measurement, the number of minimal required measurements $N=2$ can be computed.

$$
\begin{eqnarray}
    N &\ge& \frac{S}{I} \\
        &\ge& \frac{\log_2{9}}{\log_2(3)} \\
        &\ge& \frac{\log_2(3^2)}{\log_2(3)} \\
        &\ge& 2\frac{\log_2(3)}{\log_2(3)} \\
        &\ge& 2
\end{eqnarray}
$$

### d) experiment
> In order to actually identify the defective coin in two weightings, you need to ensure that each weighing yields exactly I bits of information. Construct a protocol to do this.

First, the coins are distributed into $3$ groups of $3$ coins. Two of these groups are weighted.

If one of the groups is heavier, this group is selected as containing the heavy coin. Otherwise, the non-weighted group contains it.

The group with the heavy coin is split up in $3$ single coins and the process repeats. Again, either one of the weighted coins is heavier, or the not-weighted coin from the same group is the weighted coin.

This identifies the different coin in $2$ measurements, provided the scale is exact enough.