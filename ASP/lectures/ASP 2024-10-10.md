# Advanced Statistical Physics: Lecture 2
# $I$ Microscopic and Macroscopic Degrees of Freedom

## $1^\circ$ Thermodynamics and Entropy
### $1.1$ [[20230829190239|Entropy]]
*[[ASP 2024-10-09|continued]]*

#### [[20241016102356|Volume Dependence: The Ideal Gas Law]]
In [[20230408112055|quantum mechanics]], counting the [[20230402162515|number of states]] $\Omega$ is easy, compared to [[20241010090350|classical mechanics]]. As an example, an [[20221104144059|ideal gas]][^1] in a box is considered. The [[20241010092336|volume]] $V$ is divided into small boxes of size $v_0$. Then, the [[20230228174954|microstate]] of particles describes the label of the box a particle is in, with $\frac{V}{v_0}$ microstates per particle. From this, $\Omega$ can be calculated.

[^1]: i.e. particles don't interact with each other

$$
\begin{eqnarray}
    \Omega &=& \left(\frac{V}{v_0}\right)^N \cdot f(E,N) \\
    \Rightarrow S &=& k_B \left( N \ln\frac{V}{v_0} + \ln f(E,N) \right) \\
    P &=& T\frac{\partial S}{\partial V} \\
        &=& \frac{k_BTN}{V}
\end{eqnarray}
$$

This proves the [[20230829120012|ideal gas law]].

#### Energy Dependence
With $3N$ variables, the [[20241016102556|energy shell]] can be calculated. The area of the energy shell is roughly the volume of the sphere, which is proportional to $(2mE)^{\frac{3N}{2}}$. From this, we can calculate the relation between $\Omega$ and $E$.

$$
\begin{eqnarray}
    E &=& \sum_{i=1}^N \frac{1}{2m} |p_i|^2 \\
    \Leftrightarrow \frac{1}{2m} |p_i|^2 &=& 2mE \\
    \Rightarrow \Omega &\approxeq& V^N E^{\frac{3N}{2}} \cdot \tilde f(N)
\end{eqnarray}
$$

#### Homogeneity Relation
Since $S$ is an extensive function of [[20230901205542|extensive state variables]], following must be true.

$$
\begin{eqnarray}
    \forall \lambda: S(\lambda E, \lambda V, \lambda N) &=& \lambda S(E,V,N)
\end{eqnarray}
$$

#### [[20230403101223|Sackur-Tetrode Formula]]
From the Homogeneity Relation, we can calculate the dependence of $N$, using the Energy Dependence and the Homogeneity Relation.

$$
\begin{eqnarray}
    S &=& k_B N \left(
            \ln\frac{V}{N} + \frac{3}{2} \ln \frac{E}{N} + \mathrm{const}
        \right)
\end{eqnarray}
$$

#### [[20230902132830|equipartition theorem]]
$$
\begin{eqnarray}
    \frac{\partial S}{\partial E} &=& \frac{3}{2}\frac{k_BN}{E} = \frac{1}{T} \\
    \Rightarrow E &=& \frac{3}{2} k_BTN
\end{eqnarray}
$$

#### [[20241017091536|Gibbs Paradox]]
If particles are treated as distinguishable, $\Omega$ is larger by $N!$. Thus, entropy gets an additional term $\ln N!\approx N\ln N$, which is not extensive. This is known as Gibbs Paradox.[^2]

[^2]: cf: Daan Frenkel (2014) Why colloidal systems can be described by statistical mechanics: some not very original comments on the Gibbs paradox, Molecular Physics, 112:17, 2325-2329, DOI [10.1080/00268976.2014.904051](https://doi.org/10.1080/00268976.2014.904051)

### $1.2$ [[20241017093320|Ensembles and Fluctuations]]
[[20230403100902|Boltzmann's entropy]] applies to closed systems with fixed values of $E,V,N$.

Now, we consider a macroscopic system as part of a larger system, referred to as [[20241017092452|reservoir]]. The system can exchange energy and particles with the reservoir.

![grand-canonical system](../media/Advanced_Statistical_Physics/grand_canonical_system.jpg)

This leads from the [[20230301092757|micro-canonical ensemble]] to the [[20250212122215|canonical ensemble]] and the [[20250212122508|grand-canonical ensemble]].

#### [[20250212123137|statistical Ensembles]]
For large systems, following [[20230301091417|postulate]] can be made. However, it's not entirely true.

> All accessible microstates of a closed system occur with equal probability.

Thus, the probability $P(r)$ for the system to be in any particular microstate $r$ is as follows:

$$
\begin{eqnarray}
    P(r) &=& \frac{1}{\Omega}
\end{eqnarray}
$$

The probability $P_s$ to find the (sub)system in a certain, specified microstate $(\Omega_s=1)$ with energy $E_s$ and particle number $N_s$ can be computed. The total energy $E=E_s+E_\mathrm{res}$ and total particle number $N=N_s+N_\mathrm{res}$ must be conserved.

$$
\begin{eqnarray}
    P_s &=& \frac{\Omega(E_\mathrm{res}-E_s, N-N_s)}{\Omega(E,N)}
\end{eqnarray}
$$

Since the reservoir is very large, a Taylor expansion with $E_s\ll E_\mathrm{res}$ and $N_s\ll N_\mathrm{res}$ leads to the canonical and the grand canonical ensemble.

##### [[20250212122215|canonical ensemble]]
The canonical ensemble describes a system able to exchange energy, but unable to exchange particles.

$$
\begin{eqnarray}
    P_s &=& \frac{1}{Z} \exp(-\beta E_s) \\
    Z &=& \sum_s \exp(-\beta E_s) \qq{partition function} \\
    \beta &=& \frac{1}{k_BT}
\end{eqnarray}
$$

##### [[20250212122508|grand-canonical ensemble]]
The grand canonical ensemble describes a system able to exchange energy and particles.

$$
\begin{eqnarray}
    P_s &=& \frac{1}{Y} \exp(-\beta (E_s-\mu N_s)) \\
    Y &=& \sum_s \exp(-\beta (E_s-\mu N_s)) \qq{grand partition function}
\end{eqnarray}
$$

#### Moments of extensive qualities
Moments of $E_s, N_s$ are obtained by taking the derivation of $\ln Z$ or $\ln Y$, respectively. The expectation value $\braket{E_s}$ is the thermodynamic energy for large systems. Similarly, the variance $(\Delta E_s)^2$ can be computed. Such, $Z$ is a [[20241014144201|cumulant-generating function]].

$$
\begin{eqnarray}
    \braket{E_s} &=& \frac{1}{Z} \sum_s E_s \exp(-\beta E_s) \\
        &=& - \frac{\partial}{\partial \beta} \ln Z \\
        &=& -\frac{1}{Z} \frac{\partial}{\partial \beta} \sum_s \exp(-\beta E_s) \\
    (\Delta E_s)^2 &=& \braket{(E_s-\braket{E_s})^2} \\
        &=& \frac{\partial^2}{\partial \beta^2} \ln Z \\
        &=& k_B T^2 \left(\frac{\partial E}{\partial T}\right)_{V,N} \\
        &=& k_B T^2 C_V
\end{eqnarray}
$$

The [[20230829171506|heat capacity]] $C_V$ is related to the variance of the energy, i.e. to the relative fluctuations of energy. Similarly, the [[20241011110411|compressibility]] $\kappa_T$ is related to the fluctuations of particle numbers $N$. This, too, enforces positive values for $C_V,\kappa_T$.

$$
\begin{eqnarray}
    (\Delta N)^2 &=& \frac{N^2}{V} k_BT \kappa_T
\end{eqnarray}
$$

In order of magnitude, the fluctuations are proportional to the number of particles.

$$
\begin{eqnarray}
    \Delta E &\propto& N \\
    \Delta N &\propto& N \\
    \Rightarrow \frac{\Delta E}{E} &\approx& \frac{1}{\sqrt{N}} \\
    \Rightarrow \lim_{N\rightarrow\infty} \frac{\Delta E}{E} &=& 0
\end{eqnarray}
$$

## $1.3$ Entropy and Information
Assume a system composed of $M\gg1$ identical subsystems. Each subsystem is in a [[20230228174954|microstate]] $s$ with a [[20221021130053|probability]] $P^{(s)}$.

With $M\rightarrow \infty$, the number of subsystems $m_s$ in microstate $s$ is $m_s\approx M\cdot P^{(s)}$.

The total number of microstates of the whole system is with the [[20241014150812|multinomial coefficient]], which is comparable to the [[20240521172857|binomial coefficient]], but for more than two distinct states.

$$
\begin{eqnarray}
    \Omega_\mathrm{tot} &=& \frac{M!}{\Pi_s (m_s!)} \\
    \sum_s m_s &=& M
\end{eqnarray}
$$

### [[20250212161353|Gibbs entropy]]
The Boltzmann entropy $S_\mathrm{tot}$ leads to Gibbs Entropy $S[P_s]$:

$$
\begin{eqnarray}
    S_\mathrm{tot} &=& k_B \ln \Omega_\mathrm{tot} \\
    M\rightarrow\infty:\qquad &\approx& -M k_B \sum_s P_s \ln P_s \\
    S[P_s] &=& -k_B \sum_s P_s \ln P_s
\end{eqnarray}
$$

In the [[20230301092757|micro-canonical ensemble]]:
$$
\begin{eqnarray}
    P_s &=& \frac{1}{\Omega} \\
    S &=& k_B \ln \Omega
\end{eqnarray}
$$

### [[20250212165356|Shannon Entropy]]
Shannon developed his entropy regarding information, unrelated to physics or thermodynamics in particular.

Shannon entropy quantifies the information contained in a probability distribution ${P_s}$. He found, $S[P_s]$ is the unique information measure that satisfies certain plausible postulates. In particular, the gained information shall be largest in an uniform distribution.

#### [[20250212165920|Jaynes' principle]]: Maximum Entropy method
A priori distributions should be selected such that Shannon entropy is maximized under the constraints.

*[[ASP 2024-10-16|to be continued ...]]*