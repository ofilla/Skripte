# Inference & Machine Learning: Adv. Seminar 5
# Bayesian Inference
by Oliver Filla
### Determinism & Predictability
#### [[20241111144229|determinism]]
Determinism is the philosophical concept that every event, action, or state of affairs is determined by preceding events or natural laws, making them inevitable.

Especially the [[20241010090350|classical mechanics]] considers the universe *deterministic*. In contrast, the [[20230408112055|quantum mechanics]] determines the universe as *non-deterministic* on microscopic level. 

#### [[20241111144345|predictability]]
At least in [[20241010090350|classical mechanics]], the world is considered *deterministic*. In fact, there are oft problems with predicting a specific outcome.

Except for [[20230408112055|quantum mechanics]], the reasons are limited knowledge. E.g. for a coin toss we need initial velocity, (angular) momentum, friction and more. Usually, we don't know all these parameters.

On *macroscopic systems*, or *open systems*, there are even more unknown parameters. [[20221020201709|Statistical physics]] can predict [[20230228175311|macrostates]], but no [[20230228174954|microstates]]. Another problem can be controlling parameters, especially in *chaotic systems*, where minimal changes in *Initial Conditions* have mayor, even extreme, effects on the outcome.

#### [[20241111144543|randomness]]
A random event can be defined as an "intrinsically unpredictable" $[1]$ event.

This is opposed to pseudo-random events, which are theoretically predictable. The latter may be unpredictable in practice, e.g. because (some) variables are unknown or the system is chaotic.

### [[20221021130053|probability]]
The [[20221021130053|probability]] $P(\vec x; \vec\theta)$ is used to [[20241111144345|predict]] states in the future and is described by parameters $\vec\theta$. A probability can be [[20240820145544|discrete]] or [[20240820145618|continuous]].

#### [[20241111150838|frequentist approach]]
The frequentist interpretation describes [[20221021130053|probability]] using distributions of [[20221028195435|random variables]]. A probability of $6^{-1}$ is interpreted as the expectation to roll a $6$ on every 6th dice roll shows a 6, on average.

For example, a die has $6$ different results. The probability for each is $6^{-1}$, for an ideal fair die. This can be describe by the [[20240820151831|Probability Mass Function]].

Similarly, a [[20240820145618|continuous random variable]] can be described by a [[20240421183643|probability density function]], or its integrated form, the [[20240421183828|cumulative distribution function]].

#### [[20241111162839|Bayesian approach]]
The Bayesian interpretation of [[20221021130053|probability]] describes the [[20241111163510|credibility]] $C(H)$ of a hypothesis $H$. $H$ is fully believed with $C(H)=1$ fully denied with $C(H)=0$.

* A hypothesis $H$ is the "assertion that some statement is true" $[1]$.
* The credibility $C\in[0,1]$ describes the strength of belief in a hypothesis.

If we belief more in hypothesis $H_1$ than in $H_2$, than $C(H_1)>C(H_2)$. The credibility behaves similar to a [[20221021130053|probability]], also the same rules apply.

For example, $\sum_i C_i(H_i)=1$ describes the fact that we have to distribute our belief onto existing hypothesis $H_i$. Similarly, the rules for the [[20241111142939|joint probability]] and the [[20241111143419|Addition Rule for Probabilities]] apply.

In the Bayesian interpretation, the probability $P(X)$ describes the credibility of the hypothesis "event $X$ will occur". Experiments can be used to estimate this credibility.

E.g. the probability $p(6)=1/6$ is interpreted as a credibility $C(H)=1/6$ for the hypothesis $H$ that a die roll shows a $6$.

#### [[20241111133325|conditional probability]]
The *conditional probability* $P(A|B)$ describes the probability for event $A$, if the state before $A$ was $B$. $P(A|B)$ effectively filters all probabilities $P(A)$ on data wich have also $B$.

The probability $P(A\cap B)$ for $A \mathrm{\,and\,} B$ can be computed:

$$
\begin{eqnarray}
    P(A\cap  B) &=& P(A) \cdot P(B|A)
\end{eqnarray}
$$

If $A$ and $B$ are [[20221028193625|independent]], $P(A\cap B)=P(A)\cdot P(B)$ follows.

From this, the probability $P(A\cup B)$ for $A \mathrm{\,or\,} B$ can be calculated. This can be visualised with the following Venn diagram.

$$
\begin{eqnarray}
    P(A\cup B) &=& P(A) + P(B) - P(A\cap B)
\end{eqnarray}
$$

If the events $A$ and $B$ are mutually exclusive, then $P(A\cup B) = P(A) + P(B)$. Otherwise, the union $P(A\cap B)$ has to be subtracted, since it would be added twice otherwise.

![[sample_space.svg|based on \cite[p. 33]{Lawrence}]]

#### [[20241111151214|Bayes' Theorem]]
Bayes' theorem describes the procedure of updating a [[20241111163529|hypothesis]] $H$, given that new evidence $E$ is available. It is the scientific method of changing beliefs.

$$
\begin{eqnarray}
    P(H|E) &=& \frac{P(H) P(E|H)}{P(E)} \\
    P(E) &=& P(H)P(E|H) + P(\neg H)P(E|\neg H)
\end{eqnarray}
$$

The [[20241111133325|conditional probability]] $P(H|E)$ describes the [[20241106132819|likelihood]], that the hypothesis $H$ is true under the given evidence $E$.

The proof uses the conditionality for $P(A\cap B)$, which can be described in two ways.

$$
\begin{eqnarray}
    P(A\cap  B) &=& P(A) \cdot P(B|A) \\
    P(A\cap  B) &=& P(B) \cdot P(A|B)
\end{eqnarray}
$$

##### [[20241120133323|geometric interpretation]]
A square with a side length of $1$ is used to represent the total probability. On the $x$-axis, the [[20241120131233|prior]] $P_1$ of the [[20241111163529|hypothesis]] $H$ is displayed (light green column), along with the complementary probability $P_1(\neg H)$. On the $y$-axis, the [[20241111133325|conditional probabilities]] for the data $P(E|H)$ and $P(E|\neg H)$ are independently plotted. From the resulting (dark) areas, the [[20241120131359|total probability]] $P(E)$ is derived.

![[Bayes_geometric_formula.svg]]

![[Bayes_geometric.svg]]

#### [[20241113102411|relative probability]]
Often, a single [[20221021130053|probability]] itself is not very useful. If an event $X$ occurs with probability $P_X=10\,\%$ it gives only little information, since it is the same number regardless of other probabilities.

For example, $\frac{P(A)}{P(B)}=\frac{1}{2}$ tells that event $B$ happens twice as often as event $A$. This is true regardless of the absolute probabilities, e.g. for $P(A)=0.1$ and $P(B)=0.2$ and for $P(A)=0.3$ and $P(B)=0.6$.

### [[20241205133320|Updating Credibilities]]
How to update the credibility of a hypothesis $H$:

1. Assert [[20241120131233|prior]] $P_1(H)$.
2. Evaluate evidence $E$ to get the [[20241106132819|likelihood]] $P(E|H)$.
3. Calculate [[20241120131359|overall likelihood]] $P(E)$.
4. Calculate [[20241120132156|posterior]] probability $P_2(H|E)$ with [[20241111151214|Bayes' theorem]].

#### [[20241205140426|Posterior Odds ratio]]
The Posterior Odds ratio compares the [[20241113102411|relative]] [[20241120132156|posterior probabilities]] $P_2$ for two different [[20241111163529|hypothesis]] $H_{A,B}$, that are [[20241205133320|updated]] with the same data $E$.

In that case, [[20241111151214|Bayes' theorem]] is simplified by eliminating the [[20241120131359|overall likelihood]] $P(E)$.

$$
\begin{eqnarray}
    \frac{P_2(H_A|E)}{P_2(H_B|E)}
        &=& \frac{P(E|H_A)}{P(E|H_B)}
            \frac{P(H_A)}{P(H_B)}
\end{eqnarray}
$$

#### Example: rapid antigen tests
Assume a COVID rapid test with a [[20241118152744|sensitivity]] of $80\,\%$, causing $20\,\%$ [[20240418112630|false-negative]] results, and a [[20241118152542|specificity]] of $98\,\%$, causing $2\,\%$ [[20240418112610|false-positive]] results.

Assume a positive test. How likely is it, that the patient does really have COVID? This [[20241106132819|likelihood]] depends on the incidence for COVID.

![[antigentest_rki.png]]

# Literature
1. Lawrence A. (2019) *Probability in Physics: An Introductory Guide*, Springer Nature Switzerland, DOI [10.1007/978-3-030-04544-9](https://doi.org/10.1007/978-3-030-04544-9)
2. Clyde M. et al. (2022) *An Introduction to Bayesian Thinking: A Companion to the Statistics with R Course*, [GitBooks](https://statswithr.github.io/book/) / [GitHub](https://github.com/enstarprise/an-introduction-to-bayesian-thinking), on 2022-06-15
3. 3Blue1Brown (2020) *Bayes theorem, the geometry of changing beliefs*, [Youtube](https://www.youtube.com/watch?v=HZGCoVF3YvM), 2024-11-16
4. Kozyrkov C. (2020) *Are you Bayesian or Frequentist?* [YouTube](https://www.youtube.com/watch?v=HZGCoVF3YvM), 2024-11-16
