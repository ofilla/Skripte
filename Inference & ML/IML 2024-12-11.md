# Inference & Machine Learning: Adv. Seminar 8
# Interpretability
aliases: Explainability, Transparency

Interpretierbarkeit bedeutet im [[20241023125139|statistischen Lernen]] das Verständnis bzw. die Erklärbarkeit des statistischen Modells. Um z.B. Diskriminierung durch ML zu verhindern oder falsche Kausalitäten zu verhindern, muss das Modell interpretierbar sein. Dies ist für einen verantwortlichen Umgang notwendig.

Andererseits sollen Ergebnisse im wissenschaftlichen Kontext auch erklärbar sein.

Beispielsweise beeinflusst der Dialekt, in dem mit LLMs wie ChatGPT kommuniziert wird, das Ergebnis: Diese Modelle können rassistisch sein. Simulierte Gerichtsverfahren kamen bei Afrikanisch-Amerikanischem Dialekt zu mehr Verurteilungen, auch mit teils erheblich mehr Todesurteilen als bei "standard"-Englisch.[^1]

[^1]: Quelle: Nature (8/2024): 

### Interpretability
Definitions:
1. Humans are ultimately the consumers of the explanations.
2. Interpretability is the degree to which a human can understand the cause of a prediction,
3. or the degree to which a human can consistently predict the model's result.

Es wird zwischen *intrinsical interpretable models* und *post hoc explanation* unterschieden
## interpretable machine learning in science
ML is not a replacement for scientific work, but a tool.

## intrinsical interpretable models
Intrinsisch interpretierbare Modelle beschränken die Komplexität des statistischen Modells. z.B. kleine Entscheidungsbäume gehören dazu, ebenso sparse linear models, Lasso, symbolic regression und SINDy.

Diese Modelle werden auch als whitebox models bezeichnet, da man in das Modell "hineinsehen" kann.

## post hoc explanation, model agnostic
Manche statistische Modelle, z.B. neurale Netzwerke, sind nicht *intrinsisch interpretierbar*. Stattdessen gibt es Modell-agnostische Methoden, um Interpretability zu erlangen.

Dies ist für sog. blackboxes notwending.

#### model agnostic
Ein weiteres ML Modell wird hinter das ursprüngliche ML modell geschaltet. Dieses soll das Interpretierbarkeit eines blackbox Modells ermöglichen.

Beispiele sind SHAP, LIME (Local Interpretable Model-Agnostic Explanations) oder PDP (Partial Dependence Plot).

## global- and local explainability
Local Explanantion: Erklärt einen einzelnen Datenpunkt, dafür sehr detailliert. Dies könnte z.B. für in der Medizin sinnvoll sein, indem einzelne Einflussfaktoren für einen einzelnen Patienten analysiert werden.

Global Explanation: Interpretiert das gesamte Modell in Bezug auf generelle Logik. Dies ist beispielsweise für wissenschaftliche Anwendungen sinnvoll.

## outlook: SHAP
$f(\mathrm{input})$ beschreibt das Ergebnis der Vorhersage, das durch die mittlere Vorhersage $\bar f$ und eine Summe an SHAP-Values $\phi_i$ beschrieben wird.

Die Sensitivität $s$ dagegen beschreibt den [[20231102164222|Gradienten]] der [[20241106150836|Features]], die die Veränderung von einzelnen Parametern beschreibt.

$$
\begin{eqnarray}
    f(\mathrm{input}) &=& \bar f + \sum_i \phi_i \\
    s &=& \left.\pdv{f(\vec x)}{x_j}\right|_{\vec x = \vec x_\mathrm{local}}
\end{eqnarray}
$$
