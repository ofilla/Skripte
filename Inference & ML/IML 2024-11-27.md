# Inference & Machine Learning: Adv. Seminar 6
# SINDy: Sparce Identification of Nonlinear Dynamics
Complicated measures, like the trajectory of falling objects, can be learned by a *Deep Neural Network*, that could predict other falling objects. This model is a *blackbox*, which we cannot understand.

A [[20230903105431|differential equation]] can archive the same result, but has more potential for *generalisation*. This can be done by following method.

1. Extract a trajectory and the corresponding velocity. From this, the dynamics follows.
2. Find the correct library of variables $\Theta$.
3. Sparse optimisation yields a set of weights $\Xi$.

$$
\begin{eqnarray}
    \dot x(t_i) &=& f(\Theta(t_i)) \\
    \qq*{e.g.} \dot x(t_i) &=& \sum_{i=1}^n w_i\Theta_i(t_i) \\
    \pmatrix{
            \dot x(t_1)\\
            \vdots\\
            \dot x(t_n)
        } &=&
            \pmatrix{
                \Theta_1(t_1) & \dots & \Theta_n(t_1) \\
                \Theta_1(t_2) & \dots 
            }
            \cdot \pmatrix{w_1\\w_2\\\vdots}
\end{eqnarray}
$$


To make this interpretable, most $\xi_i$ must be eliminated.

### library of variables
A library of variables $\Theta$ is a set of functions dependent on the variables of the data. For example the coordinate in one dimension could have $\Theta=[1, x, x^2, \dot x, \sin(x), \sinh(x)]$.

The library should be as slim as possible, also due to computational limits.

For example, for a free fall, the model is given as follows, with an error $\epsilon$. This yields to data points $s(t_i)$.

$$
\begin{eqnarray}
    s(t) &=& s_0 + v_0 t + \frac{1}{2}gt^2 + \epsilon
    s(t_i) &=& s_0 + v_0 t_i + \frac{1}{2}gt_i^2 + \epsilon_i \\
    \pmatrix{
            s(t_1) \\ \vdots \\ s(t_n)
        } &=&
            \pmatrix{
                1 & t_1 & t_1^2 \\
                \vdots & \vdots & \vdots \\
                1 & t_n & t_n^2
            }
            \cdot \pmatrix{s_0 \\ v_0 \\ g}
\end{eqnarray}
$$

If the model is not known before, additional dependencies are possible.

$$
\begin{eqnarray}
    \pmatrix{
            s(t_1) \\ \vdots \\ s(t_n)
        } &=&
            \pmatrix{
                1 & t_1 & t_1^2 & t_1^3 & \dots & \cos(t_1) \\
                \vdots & \vdots & \vdots \\
                1 & t_n & t_n^2 & t_n^3 & \dots & \cos(t_n)
            }
            \cdot \pmatrix{s_0 \\ v_0 \\ g \\ \xi_4 \\ \vdots \\ \xi_n}
\end{eqnarray}
$$

### How many data are required?
The higher the sampling rate, the less sampling length is required. Also, the quality of data is important.

If the data are noisy, a small sampling rate can yield to an approximate derivative. Also, de-noising techniques can be applied. Sometimes, artificial data points can be "simulated", if not enough data are available.

### guided model discovery
SINDy is a tool for guided model discovery. Prior knowledge can help with an adjudicated guess, which library of variables is useful.

### Sequential threshold of least squares
It is a method to get a weight vector $\Xi$ sparse.

First, a [[20241106141953|least squares]] regression is applied. Next, all terms under a certain threshold are eliminated, i.e. set to $0$. Repeating this yields to a sparse vector. This circle is stopped either the error on the [[20241111152624|test data]] increases or if a predefined number of parameters is reached, e.g. $3$ parameters are left.

# Symbolic Regression
Symbolic regression is a more general regression method than e.g. *SINDy* or the *Lasso* method. It looks form explicitly functions and selects the simplest function describing the data.

With complex data or in high dimensions, symbolic regression does not work well. Instead, it takes an eternity to run.

Python Package: PySR; uses Julia Module SymbolicRegression

## Method
In general, many functions ($\ge 10^2$) are generated simultaneously. For each function, mutational operators are applied. E.g. two nodes can be swapped or a node could be dropped.

Next, the [[20241023133144|loss]] $f_l$ is evaluated, after that, the complexity is evaluated. Complexity gets assigned a penalty $f_c$, e.g. the $\ell_1$-norm or the $\ell_2$-norm.

The total score is a combination of loss and complexity score.

## Why using symbolic regression?
With polynomial regression, the highest order is likely to give the best results. But this result is not interpretable. For example, a part of a  $\sin$ function could be fitted by e.g. a polynomial of degree $13$ or higher.

Another approach is to take a model, give it a set of operations (like $+$, $-$, $\cdot$, $/$) and a set of functions (like $x$, $x^2$, $\sin x$). The model finds the best function, leading to an interpretable result.

If a model is known, *SINDy* is faster or better. However, 

## Function Tree
A function [[20230804155842|tree]] is a [[20230805123136|data type]] representing a function.

