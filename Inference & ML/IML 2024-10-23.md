# Inference & Machine Learning: Adv. Seminar 2
# Fundamentals of Machine Learning
## [[20241023125139|statistical learning]]
Statistical Learning builds a statistical model based on a collection of sample data points, to predict or estimate outputs based on inputs.

In general, there is a trade-off between [[20241023125232|inference]] and [[20241023125455|prediction]] accuracy.

There are three types of [[20241023125139|statistical learning]]: [[20241023132426|supervised learning]], [[20241023132449|unsupervised learning]] and [[20241023132509|semi-supervised learning]], a mixture from the other two types.

A [[20241023133144|loss function]] describes the inaccuracy of the model. It is to be minimized.

For example, the [[20230901142707|mean squared error]] is the loss function in [[20230901151622|linear regression]].

## [[20241023133734|Model Testing and Generalisation]]
The [[20241023133541|Train-Test Split]] splits up known data into a training set and a testing set.
This is used to minimize [[20241023133928|overfitting]].

The train data can be scaled and normalized. Further, outliers and missing values can be handled and data can be balanced as wanted.

There is a trade-off between [[20241023134453|bias]] and [[20221028194035|variance]].

#### [[20221028194035|variance]]
The variance describes the sensitivity to small fluctuations. If the model has a high variance, it can be [[20241023133928|overfitted]].

#### [[20241023134453|bias]]
In [[20241023125139|statistical learning]], the bias describes the generalization abilities of a model. If the bias is high, the model can be [[20241023134053|underfitted]]. In this case, the model cannot predict (most) training data correctly.

## [[20241023134625|regularization]]
Regularization is used to optimize a [[20241023125139|statistical model]]. It can reduce the dimension of important parameters. The amount and weight of single parameters can be used for the estimation of the model, penalizing weights.

The regularization $r(\vec \beta)$ modifies the [[20241023133144|loss function]].

$$
\begin{eqnarray}
    f_\mathrm{loss}^\prime &=& f_\mathrm{loss} + \frac{r(\vec{\beta})}{C}
\end{eqnarray}
$$

## [[20241023134726|hyperparameter optimazation]]
Hyperparameters describe parameters set outside of the model. They are not part of the learning process, but an optimization method, and tune the [[20241023134625|regularization]] strength.

Hyperparmeters $C$ modify the [[20241023133144|loss function]].

$$
\begin{eqnarray}
    f_\mathrm{loss}^\prime &=& f_\mathrm{loss} + \frac{r(\vec{\beta})}{C}
\end{eqnarray}
$$

To validate the optimal hyperparameter, validation data can be selected from the train data, testing the model. One method is the [[20241111153434|k-fold cross validation]].
