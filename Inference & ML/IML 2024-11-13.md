# Inference & Machine Learning: Adv. Seminar 4
# Sparsity and the Lasso
Example: What contributes to one's success in physics study? Take the final grade as an output variable, what input variables can relate?

Not all variables are equally important, some may be completely irrelevant.

## Sparsity
A sparse model involves many variables, but only a subset of variables are relevant. Similarly, a vector or a matrix is sparse if it contains many $0$s.

Sparsity can be archived with [[20241023134625|regularization]]. If a model is not sparse enough, it can be [[20241023133928|overfitted]].

## parsimony principle / Ockham's razor
We choose the simples possible model, if we have a choice, since it is most likely to be correct.

## The Lasso Regression
The Lasso is a [[20230901151622|linear regression]], modifying the [[20241106141953|method of least squares]]. It typically finds the sparsest fitting model, for a model linearly dependent on the model's parameters. It can decrease the [[20221028194035|variance]] with a small increase in [[20241023134453|bias]], which increases *Generalisation*. Furthermore, it can increase the interpretability of the model.

The standard linear model is $Y=\beta_0 + \sum_i \beta_iX_i + \epsilon$, where $\epsilon$ is an error term. This formula can be described with a matrix. Applying the *parsimony principle*, the parameter vector $\vec\beta$ should become sparse. Directly optimizing sparsity is computational impossible, for large models.

The [[20241023133144|loss function]] $L$ generalizes the [[20241106141953|method of least squares]]. The [[20241011124234|method of Lagrange multipliers]] is applied to MLS with the constraint of the $\ell_1$-Norm to be minimal $||\vec \beta||_1\le s$, with small $s$. The Lagrange multiplier $\lambda$ is the inverse [[20241023134726|hyperparameter]] $\lambda^{-1}$.

Also, the Lasso follows from [[20241111151214|Bayes' Theorem]], if the [[20241120131233|prior]] assumes a [[20230223132329|Gaussian distribution]] in the noise. This can be motivated with the [[20230223130500|central limit theorem]].

## The $\ell_1$-Norm
Die $\ell_1$-Norm wird in der *Lasso Regression* verwendet, um ein möglichst *dünn-besetztes* *statistisches Modell* zu erhalten.

$$
\begin{eqnarray}
    \norm{\vec\beta}_1 &=& \sum_{j=1}^n |\beta_i|
\end{eqnarray}
$$
